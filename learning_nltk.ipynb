{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Data Preprocessing Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a random text we are using to demonstrate the implementation of the tokenization.', 'Please follow the course']\n"
     ]
    }
   ],
   "source": [
    "# sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = 'This is a random text we are using to demonstrate the implementation of the tokenization. Please follow the course'\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'random', 'text', 'we', 'are', 'using', 'to', 'demonstrate', 'the', 'implementation', 'of', 'the', 'tokenization', '.', 'Please', 'follow', 'the', 'course']\n"
     ]
    }
   ],
   "source": [
    "# word tokenization example\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barack\n",
      "hussein\n",
      "obama\n",
      "ii\n",
      "born\n",
      "august\n",
      "american\n",
      "politician\n",
      "served\n",
      "president\n",
      "united\n",
      "states\n",
      "member\n",
      "democratic\n",
      "party\n",
      "first\n",
      "president\n",
      "history\n",
      "obama\n",
      "previously\n",
      "served\n",
      "senator\n",
      "representing\n",
      "illinois\n",
      "illinois\n",
      "state\n",
      "senator\n",
      "civil\n",
      "rights\n",
      "lawyer\n",
      "university\n",
      "lecturer\n",
      "obama\n",
      "born\n",
      "honolulu\n",
      "hawaii\n",
      "graduated\n",
      "columbia\n",
      "university\n",
      "political\n",
      "science\n",
      "later\n",
      "worked\n",
      "community\n",
      "organizer\n",
      "chicago\n",
      "obama\n",
      "enrolled\n",
      "harvard\n",
      "law\n",
      "school\n",
      "first\n",
      "black\n",
      "president\n",
      "harvard\n",
      "law\n",
      "review\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Load the NLTK stop words\n",
    "# nltk.download(\"stopwords\")            # used only once\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Read the content from the file \"obama.txt\"\n",
    "with open(\"obama.txt\", \"r\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Tokenize the content into words\n",
    "words = nltk.word_tokenize(content)\n",
    "\n",
    "# Remove punctuation and convert to lowercase\n",
    "filtered_words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "# Remove stop words\n",
    "filtered_words = [word for word in filtered_words if word not in stop_words]\n",
    "\n",
    "\n",
    "for word in filtered_words:\n",
    "    print(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let\n",
      "me\n",
      "excit\n",
      "you\n",
      "with\n",
      "the\n",
      "excit\n",
      "i\n",
      "got\n",
      "when\n",
      "the\n",
      "trainer\n",
      "wa\n",
      "train\n",
      "us\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "set(stopwords.words('English'))\n",
    "ps = PorterStemmer()\n",
    "text='Let me excite you with the excitement I got when the trainer was training us'\n",
    "words = word_tokenize(text)\n",
    "for w in words:\n",
    "    print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemmatized words:  ['Let', 'me', 'excite', 'you', 'with', 'the', 'excitement', 'I', 'got', 'when', 'the', 'trainer', 'wa', 'training', 'u']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words] \n",
    "print(\"The lemmatized words: \", lemmatized_words) #prints the lemmatized words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Let', 'VB'), ('me', 'PRP'), ('excite', 'VB'), ('you', 'PRP'), ('with', 'IN'), ('the', 'DT'), ('excitement', 'NN'), ('I', 'PRP'), ('got', 'VBD'), ('when', 'WRB'), ('the', 'DT'), ('trainer', 'NN'), ('wa', 'VBD'), ('training', 'NN'), ('u', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# POS Tagging\n",
    "from nltk.tag import DefaultTagger\n",
    "tagged = nltk.pos_tag(lemmatized_words)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMED ENTITY\t\tTYPE\n",
      "===================================\n",
      "Barack               (PERSON)\n",
      "Hussein Obama II     (PERSON)\n",
      "American             (GPE)\n",
      "United States        (GPE)\n",
      "Democratic Party     (ORGANIZATION)\n",
      "U.S.                 (GPE)\n",
      "Obama                (PERSON)\n",
      "U.S.                 (GPE)\n",
      "Illinois             (PERSON)\n",
      "Obama                (PERSON)\n",
      "Honolulu             (GPE)\n",
      "Hawaii               (GPE)\n",
      "Columbia University  (ORGANIZATION)\n",
      "Chicago              (GPE)\n",
      "Obama                (PERSON)\n",
      "Harvard Law School   (ORGANIZATION)\n",
      "Harvard Law Review   (ORGANIZATION)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "# Read the text from 'obama.txt' file\n",
    "with open('obama.txt', 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = word_tokenize(text)\n",
    "\n",
    "# Part-of-speech tagging\n",
    "tagged_text = pos_tag(tokenized_text)\n",
    "\n",
    "# Extract named entities\n",
    "def extract_named_entities(tagged_text):\n",
    "    named_entities = []\n",
    "    for chunk in ne_chunk(tagged_text):\n",
    "        if isinstance(chunk, Tree):\n",
    "            entity_name = ' '.join([token for token, pos in chunk.leaves()])\n",
    "            entity_type = chunk.label()\n",
    "            named_entities.append((entity_name, entity_type))\n",
    "    return named_entities\n",
    "\n",
    "# Get named entities from the text\n",
    "named_entities = extract_named_entities(tagged_text)\n",
    "\n",
    "# Print the named entities\n",
    "print('NAMED ENTITY\\t\\tTYPE')\n",
    "print('='*35)\n",
    "for entity_name, entity_type in named_entities:\n",
    "    print(f\"{entity_name:{20}} ({entity_type})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
